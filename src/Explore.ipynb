{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch as th\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from sklearn.utils import gen_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bodies_data = pd.read_csv('data/train_bodies.csv')\n",
    "train_stances_data =  pd.read_csv('data/train_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(stances, bodies, id=None, stance='unrelated'):\n",
    "    if id is None:\n",
    "        headline = stances[stances['Stance'] == stance].sample().iloc[0]\n",
    "    else:\n",
    "        headline = stances[stances['Body ID'] == id].iloc[0]\n",
    "\n",
    "    body = bodies[bodies['Body ID'] == headline['Body ID']].iloc[0]\n",
    "    return {\n",
    "        'headline': headline['Headline'],\n",
    "        'id': headline['Body ID'],\n",
    "        'body': body['articleBody'],\n",
    "        'stance': headline['Stance']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_sent_length(data):\n",
    "    sent_lengths=[]\n",
    "    for sent in data:\n",
    "        sent_lengths.append(len(sent))\n",
    "    max_length = max(sent_lengths)\n",
    "\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(data):\n",
    "    max_length = get_max_sent_length(data)\n",
    "    for sent in data:\n",
    "        while len(sent) < max_length:\n",
    "            sent.append('<pad>')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(stances_data, bodies_data):\n",
    "    headlines = []\n",
    "    bodies = []\n",
    "    stances = []\n",
    "    for stance in stances_data.iterrows():\n",
    "        stance = stance[1]\n",
    "        headlines.append(stance['Headline'])\n",
    "        body = bodies_data[bodies_data['Body ID'] == stance['Body ID']].iloc[0]\n",
    "        bodies.append(body['articleBody'])\n",
    "        stances.append(stance['Stance'])\n",
    "        \n",
    "    return headlines, stances, bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    return list(map(clean_str, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_dict = {\n",
    "    'agree': 0,\n",
    "    'disagree': 1,\n",
    "    'discuss': 2,\n",
    "    'unrelated': 3\n",
    "}\n",
    "\n",
    "def transform_stances(stances):\n",
    "    return [stance_dict[stance] for stance in stances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headlines, train_stances, train_bodies = join_data(train_stances_data, train_bodies_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headlines_clean = clean_data(train_headlines)\n",
    "train_stances_clean = transform_stances(train_stances)\n",
    "train_bodies_clean = clean_data(train_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "headlines_train, headlines_dev, stances_train, stances_dev, bodies_train, bodies_dev = train_test_split(\n",
    "    train_headlines_clean,\n",
    "    train_stances_clean,\n",
    "    train_bodies_clean,\n",
    "    test_size=0.2,\n",
    "    stratify=train_stances_clean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict['<pad>'] = 0\n",
    "    \n",
    "    i = 1\n",
    "    for sent in data:\n",
    "        for word in sent:\n",
    "            if word not in vocab_dict:\n",
    "                vocab_dict[word] = i\n",
    "                i += 1\n",
    "                \n",
    "    vocab_dict['<unk>'] = i\n",
    "    \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = create_vocabulary(headlines_train + bodies_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_num_tensor(data):\n",
    "    data = [sent.split() for sent in data]\n",
    "    data = fill_data(data)\n",
    "    num_tensor = []\n",
    "    for sent in data:\n",
    "        new_sent=[]\n",
    "        for word in sent:\n",
    "            new_sent.append(vocab_dict[word] if word in vocab_dict else vocab_dict['<unk>'])\n",
    "        num_tensor.append(new_sent)\n",
    "    return th.tensor(num_tensor, dtype=th.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_train = data_to_num_tensor(headlines_train)\n",
    "bodies_train = data_to_num_tensor(bodies_train)\n",
    "headlines_dev = data_to_num_tensor(headlines_dev)\n",
    "bodies_dev = data_to_num_tensor(bodies_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stances_train = th.tensor(stances_train, dtype=th.int64)\n",
    "stances_dev = th.tensor(stances_dev, dtype=th.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_dict, embedding_dim, num_layers=0, hidden_dim=50, dropout=0.5):\n",
    "        super(CBOW_classifier, self).__init__()     \n",
    "        output_dim = 4\n",
    "        self.embedding = nn.Embedding(len(vocab_dict), embedding_dim, padding_idx=vocab_dict['<pad>'])\n",
    "        if num_layers > 0:\n",
    "            first_layer = nn.Sequential(nn.Linear(2*embedding_dim, hidden_dim),nn.ReLU())\n",
    "            hidden_layers = [nn.Sequential(nn.Linear(hidden_dim, hidden_dim),nn.ReLU()) for i in range(num_layers-1)]\n",
    "            self.out = nn.Sequential(nn.Dropout(dropout), first_layer, *hidden_layers, nn.Dropout(dropout), nn.Linear(hidden_dim, output_dim))\n",
    "        else:\n",
    "            self.out = nn.Sequential(nn.Dropout(dropout),nn.Linear(2*embedding_dim, output_dim))        \n",
    "        \n",
    "    def forward(self, headlines, bodies):\n",
    "        headlines_embedded = th.sum(self.embedding(headlines), axis=1)\n",
    "        bodies_embedded = th.sum(self.embedding(bodies), axis=1)\n",
    "        embeds = th.cat((headlines_embedded, bodies_embedded), 1)\n",
    "        out = self.out(embeds)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, headlines, stances, bodies):\n",
    "    with th.no_grad():\n",
    "        outputs = model.forward(headlines, bodies).argmax(axis=1)\n",
    "        accuracy = (outputs == stances).sum().float() / stances.numel()\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "model = CBOW_classifier(vocab_dict, embedding_dim, num_layers=0, dropout=0)\n",
    "lr = 0.008\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 20\n",
    "num_samples = len(headlines_train)\n",
    "\n",
    "slices = list(gen_batches(num_samples, batch_size))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for s in slices: \n",
    "        headlines_batch = headlines_train[s]\n",
    "        stances_batch = stances_train[s]\n",
    "        bodies_batch = bodies_train[s]\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        pred_labels = model(headlines_batch, bodies_batch)\n",
    "        loss = loss_function(pred_labels, stances_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    acc = compute_accuracy(model, headlines_dev, stances_dev, bodies_dev)\n",
    "    trainacc = compute_accuracy(model, headlines_train, stances_train, bodies_train)\n",
    "    print('Epoch:', epoch, \"Accuracy: %f\" % acc, \"Train accuracy: %f\" % trainacc)\n",
    "    print('\\tLoss:', epoch_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
